# Python Scripts for Distribution Analysis and Data Processing

## Overview

This project analyzes the accuracy of various project estimation methods (CPM, PERT, BB/2 Mean, Lognormal Mean) against simulated completion times for different probability distributions (Triangular, Lognormal, Beta). The analysis calculates average percent differences between estimated and simulated values across multiple datasets.

The project has evolved to include comprehensive data preparation, cleaning, and combination steps to standardize and unify the simulation results for robust analysis.

## Project Structure

### Folders

- **Code/**: Main scripts directory
  - **EDA/**: Exploratory Data Analysis scripts (e.g., Percent_Difference_Method.py)
  - **File Combination/**: Data cleaning and combination scripts
- **Simulation Results/**: Original simulation data (Beta, Lognormal, Triangular subfolders)
- **Simulation Results copy/**: Working copy of simulation data with modifications
  - **Activities Beta/**: Beta distribution simulation CSVs
  - **Activities Lognormal/**: Lognormal distribution simulation CSVs
  - **Activities Triangular/**: Triangular distribution simulation CSVs
  - **Combined Datasets/**: Combined CSVs with data from all distributions
- **Ouput/**: Output files (e.g., combined_output.txt)
- **__pycache__/**: Python bytecode files

### Core Analysis Modules (EDA Folder)

- **Percent_Difference_Method.py**: Contains the `calculate_percent_differences(filepath)` function. This function reads a CSV file containing simulation results, compares estimation methods against the 'Sim Mean' reference column, and computes the average absolute percent difference for each method. It returns a dictionary of these differences.

- **Overall_Distribution_Per_Diff.py**: Contains the `test_overall_distribution_average_percent_differences(paths)` function. This function takes a list of file paths, calls `calculate_percent_differences` for each, accumulates the results, and returns the overall average differences across all files along with a list of individual file results.

### Distribution-Specific Analysis Scripts (Code Folder)

- **Triangular_Distribution_Avg_%_Diff.py**: Runs the analysis for Triangular distribution datasets. It defines a list of CSV file paths for Triangular simulation results and computes the overall average percent differences for this distribution.

- **Lognormal_Distribution_Avg_%_Diff.py**: Runs the analysis for Lognormal distribution datasets. It defines a list of CSV file paths for Lognormal simulation results and computes the overall average percent differences for this distribution.

- **BigBeta_Distribution_Avg_%_Diff.py**: Runs the analysis for Beta (BigBeta) distribution datasets. It defines a list of CSV file paths for Beta simulation results and computes the overall average percent differences for this distribution.

### Data Preparation and Combination Scripts (File Combination Folder)

- **Data Cleaning.py**: Handles data standardization and cleaning. Identifies CSV files with missing columns, adds missing columns with data from corresponding files, and ensures all files have consistent 28 columns.

- **read_csv_columns.py**: Generates a JSON file (`column_names_copy.json`) listing column names for all CSV files, used for validation and processing.

- **dataset_combination.py**: Combines corresponding CSV files from the three distribution folders into unified datasets. Each combined file stacks data from Beta, Lognormal, and Triangular distributions, adds a "Distribution" column, and includes dummy variables (Is_Beta, Is_Lognormal, Is_Triangular) for modeling.

### Utility Scripts (Code Folder)

- **Individual_File_Stats_Calculator.py**: A standalone script that prompts the user for a single file path and calculates the percent differences for that file using the `Percent_Difference_Method` module. Useful for quick analysis of individual datasets.

- **Combined_Run.py**: A comprehensive script that runs the analysis for all three distributions (Triangular, Lognormal, BigBeta) in sequence. It prints summaries to the console and writes detailed results (overall dictionaries and individual file dictionaries) to `combined_output.txt`.

## Data Processing Workflow

1. **Data Standardization**: `Data Cleaning.py` ensures all CSV files have consistent columns by adding missing data from corresponding files.

2. **Column Validation**: `read_csv_columns.py` scans all files and generates a JSON summary of column structures.

3. **Dataset Combination**: `dataset_combination.py` creates combined CSV files in "Combined Datasets/", stacking data from all distributions with identification columns.

4. **Analysis**: Original EDA scripts process the combined or individual datasets to compute percent differences.

## Output Files

- **combined_output.txt**: Generated by `Combined_Run.py`, contains complete output data including overall result dictionaries for each distribution and all individual file dictionaries.

- **column_names_copy.json**: JSON file listing column names for all processed CSV files.

- **Combined Datasets/**: Folder containing 9 combined CSV files (e.g., "Set 1 SP Completion Times - Combined.csv"), each with ~1200-2700 rows from all distributions, plus dummy variables.

## How It Works

1. **Data Input**: Analysis starts with standardized CSV files containing simulation results. Files have columns for estimation methods (CPM, PERT, etc.) and a 'Sim Mean' reference.

2. **Data Cleaning**: Missing columns are populated, and files are standardized to 28 columns.

3. **Combination**: Corresponding files from different distributions are stacked into combined datasets with distribution identifiers.

4. **Percent Difference Calculation**: For each file/dataset, absolute percent differences between estimation methods and simulated mean are calculated and averaged.

5. **Aggregation**: Results are aggregated across files and distributions for overall comparisons.

6. **Output**: Results are printed to console and saved to files in human-readable and structured formats.

Scripts use pandas for data manipulation and require correct file paths.

## Results Overview

Running `Combined_Run.py` provides a comprehensive comparison across distributions:

- **Triangular Distribution**: CPM shows the highest error (15.98%), followed by PERT (8.98%). BB/2 Mean and Lognormal Mean perform similarly well (4.31% and 4.42%).

- **Lognormal Distribution**: Similar pattern with CPM at 13.49%, PERT at 6.28%, and BB/2 Mean and Lognormal Mean at 1.56% and 1.59% respectively.

- **BigBeta (Beta) Distribution**: CPM at 9.11%, PERT performs best at 1.54%, with BB/2 Mean and Lognormal Mean at 3.94% and 3.85%.

Overall, PERT and Lognormal Mean tend to provide the most accurate estimates across distributions, while CPM consistently shows the highest percent differences. The Beta distribution yields the lowest errors for most methods, suggesting better estimation performance for this distribution type.

## Usage

1. Ensure data is in "Simulation Results copy/" with subfolders.
2. Run data cleaning: `python Code/File Combination/Data Cleaning.py`
3. Generate column summary: `python Code/File Combination/read_csv_columns.py`
4. Combine datasets: `python Code/File Combination/dataset_combination.py`
5. Run analysis: `python Code/Combined_Run.py`

Combined datasets in "Combined Datasets/" can be used for further EDA or modeling.
